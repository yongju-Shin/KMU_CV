{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98rds-2OU-Rd"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:02.550865Z",
          "iopub.status.busy": "2021-04-07T18:26:02.550259Z",
          "iopub.status.idle": "2021-04-07T18:26:02.552401Z",
          "shell.execute_reply": "2021-04-07T18:26:02.552770Z"
        },
        "id": "1c95xMGcU5_Z"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1UUX8SUUiMO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_object_detection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/tensorflow/collections/object_detection/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF Hub 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# TensorFlow Hub 객체 감지 Colab\n",
        "\n",
        "TensorFlow Hub 객체 감지 Colab에 오신 것을 환영합니다! 이 노트북에서는 이미지에서 \"즉시 사용 가능한\" 객체 감지 모델을 실행하는 단계를 안내합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRImnk_7WOq1"
      },
      "source": [
        "### 더 많은 모델\n",
        "\n",
        "[이](https://tfhub.dev/tensorflow/collections/object_detection/1) 컬렉션에는 COCO 2017 데이터세트에서 훈련된 TF 2 객체 감지 모델이 포함되어 있습니다. 현재, [tfhub.dev](tfhub.dev)에서 호스팅되는 모든 객체 감지 모델은 [여기](https://tfhub.dev/s?module-type=image-object-detection)에서 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## 가져오기 및 설정\n",
        "\n",
        "기본 가져오기부터 시작하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:02.561347Z",
          "iopub.status.busy": "2021-04-07T18:26:02.560787Z",
          "iopub.status.idle": "2021-04-07T18:26:08.198623Z",
          "shell.execute_reply": "2021-04-07T18:26:08.199054Z"
        },
        "id": "yn5_uV1HLvaz"
      },
      "outputs": [],
      "source": [
        "# 시스템 명령어 처리\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# 이미지 처리\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 파일 처리\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "# 텐서플로우 허브\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## 유틸리티\n",
        "\n",
        "다음 셀을 실행하여 나중에 필요한 일부 유틸리티를 만듭니다.\n",
        "\n",
        "- 이미지를 로드하는 도우미 메서드\n",
        "- 모델 이름과 TF Hub 핸들의 매핑\n",
        "- COCO 2017 데이터세트에 대한 인간 키포인트가 있는 튜플 목록(키포인트가 있는 모델에 필요)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:08.210856Z",
          "iopub.status.busy": "2021-04-07T18:26:08.210230Z",
          "iopub.status.idle": "2021-04-07T18:26:08.211725Z",
          "shell.execute_reply": "2021-04-07T18:26:08.212082Z"
        },
        "id": "-y9R0Xllefec"
      },
      "outputs": [],
      "source": [
        "# @title Run this!!\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n",
        "\n",
        "IMAGES_FOR_TEST = {\n",
        "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
        "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
        "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
        "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "}\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNk1gzh0TN"
      },
      "source": [
        "## 시각화 도구\n",
        "\n",
        "적절하게 감지된 상자, 키포인트 및 세분화로 이미지를 시각화하기 위해 TensorFlow Object Detection API를 사용합니다. 설치를 위해 리포지토리를 복제합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:08.221668Z",
          "iopub.status.busy": "2021-04-07T18:26:08.220944Z",
          "iopub.status.idle": "2021-04-07T18:26:12.067110Z",
          "shell.execute_reply": "2021-04-07T18:26:12.066542Z"
        },
        "id": "oi28cqGGFWnY"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX3pb_pXDjYA"
      },
      "source": [
        "Object Detection API 설치하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:12.072268Z",
          "iopub.status.busy": "2021-04-07T18:26:12.071626Z",
          "iopub.status.idle": "2021-04-07T18:26:52.902820Z",
          "shell.execute_reply": "2021-04-07T18:26:52.902384Z"
        },
        "id": "NwdsBdGhFanc"
      },
      "outputs": [],
      "source": [
        "# 리눅스 명령어\n",
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install -q .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yDNgIx-kV7X"
      },
      "source": [
        "이제 나중에 필요한 종속성을 가져올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:52.907993Z",
          "iopub.status.busy": "2021-04-07T18:26:52.907367Z",
          "iopub.status.idle": "2021-04-07T18:26:53.365068Z",
          "shell.execute_reply": "2021-04-07T18:26:53.364535Z"
        },
        "id": "2JCeQU3fkayh"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "### 플로팅을 위해 레이블 맵 데이터 로드하기\n",
        "\n",
        "레이블 맵은 인덱스 번호를 범주 이름에 대응시키므로, 컨볼루션 네트워크가 `5`를 예측할 때 이것이 `airplane`에 해당한다는 것을 알 수 있습니다. 여기서는 내부 유틸리티 함수를 사용하지만, 정수를 적절한 문자열 레이블에 매핑하는 사전을 반환하는 함수라면 문제 없습니다.\n",
        "\n",
        "간단히 하기 위해 Object Detection API 코드를 로드한 리포지토리에서 로드하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:53.369101Z",
          "iopub.status.busy": "2021-04-07T18:26:53.368523Z",
          "iopub.status.idle": "2021-04-07T18:26:53.376961Z",
          "shell.execute_reply": "2021-04-07T18:26:53.376538Z"
        },
        "id": "5mucYUS6exUJ"
      },
      "outputs": [],
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "## 감지 모델을 빌드하고 사전 훈련된 모델 가중치 로드하기\n",
        "\n",
        "여기서 사용할 객체 감지 모델을 선택합니다. 아키텍처를 선택하면 자동으로 로드됩니다. 나중에 다른 아키텍처를 시도하기 위해 모델을 변경하려면 다음 셀을 변경하고 이어지는 셀을 실행하세요.\n",
        "\n",
        "**팁:** 선택한 모델에 대한 자세한 내용을 보려면 링크(모델 핸들)로 이동하여 TF Hub에 대한 추가 문서를 읽어보세요. 모델을 선택하면 더 쉽게 처리할 수 있도록 핸들이 출력됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:53.380949Z",
          "iopub.status.busy": "2021-04-07T18:26:53.380380Z",
          "iopub.status.idle": "2021-04-07T18:26:53.383191Z",
          "shell.execute_reply": "2021-04-07T18:26:53.382767Z"
        },
        "id": "HtwrSqvakTNn"
      },
      "outputs": [],
      "source": [
        "#@title Model Selection { display-mode: \"form\", run: \"auto\" }\n",
        "model_display_name = 'Faster R-CNN ResNet50 V1 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muhUt-wWL582"
      },
      "source": [
        "## TensorFlow Hub에서 선택한 모델 로드하기\n",
        "\n",
        "여기에서는 선택된 모델 핸들만 필요하고 Tensorflow Hub 라이브러리를 사용하여 이 핸들을 메모리에 로드합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:26:53.387008Z",
          "iopub.status.busy": "2021-04-07T18:26:53.386420Z",
          "iopub.status.idle": "2021-04-07T18:27:51.108452Z",
          "shell.execute_reply": "2021-04-07T18:27:51.107851Z"
        },
        "id": "rBuD07fLlcEO"
      },
      "outputs": [],
      "source": [
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIawRDKPPnd4"
      },
      "source": [
        "## 이미지 로드하기\n",
        "\n",
        "간단한 이미지로 모델을 시도해 보겠습니다. 이를 위해 테스트 이미지 목록을 제공합니다.\n",
        "\n",
        "궁금하다면 다음과 같이 간단한 시도를 해볼 수 있습니다.\n",
        "\n",
        "- 자신의 이미지에서 추론을 실행해 봅니다. 이미지를 colab에 업로드하고 아래 셀에서 수행한 것과 같은 방식으로 로드합니다.\n",
        "- 일부 입력 이미지를 수정하고 여전히 제대로 감지되는지 확인합니다. 여기에서 간단히 이미지를 수평으로 뒤집거나 회색조로 변환하는 몇 가지 간단한 작업을 시도해 볼 수 있습니다(입력 이미지에는 여전히 3개의 채널이 있어야 함).\n",
        "\n",
        "**주의:** 알파 채널이 있는 이미지를 사용하는 경우, 모델은 3개 채널 이미지를 예상하고 알파는 4번째 채널로 계산됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:27:51.117931Z",
          "iopub.status.busy": "2021-04-07T18:27:51.117330Z",
          "iopub.status.idle": "2021-04-07T18:27:53.719527Z",
          "shell.execute_reply": "2021-04-07T18:27:53.719987Z"
        },
        "id": "hX-AWUQ1wIEr"
      },
      "outputs": [],
      "source": [
        "#@title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n",
        "selected_image = 'Beach' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
        "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
        "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n",
        "\n",
        "image_path = IMAGES_FOR_TEST[selected_image]\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "# Flip horizontally\n",
        "if(flip_image_horizontally):\n",
        "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "if(convert_image_to_grayscale):\n",
        "  image_np[0] = np.tile(\n",
        "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTHsFjR6HNwb"
      },
      "source": [
        "## 추론하기\n",
        "\n",
        "추론을 수행하려면 TF Hub 로드 모델을 호출하기만 하면 됩니다.\n",
        "\n",
        "시도해볼 수 있는 작업:\n",
        "\n",
        "- `result['detection_boxes']`를 출력하고 상자 위치를 이미지의 상자와 일치시킵니다. 좌표는 정규화된 형식(예: [0, 1] 간격)으로 제공됩니다.\n",
        "- 결과에 있는 다른 출력 키를 검사합니다. 모델 문서 페이지에서 전체 문서를 볼 수 있습니다(브라우저에서 앞서 출력된 모델 핸들을 가리킴)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:27:53.734864Z",
          "iopub.status.busy": "2021-04-07T18:27:53.724532Z",
          "iopub.status.idle": "2021-04-07T18:27:58.528151Z",
          "shell.execute_reply": "2021-04-07T18:27:58.528543Z"
        },
        "id": "Gb_siXKcnnGC"
      },
      "outputs": [],
      "source": [
        "# running inference\n",
        "# hub_model: 위에서 선택한 모델로 불러오도록 코드 구성\n",
        "# hub_model에서 이미지는 4채널로 선언해주어야 함. 따라서 맨 앞에 1차원 만들어줌\n",
        "print(image_np.shape)    #이미지 크기 확인\n",
        "results = hub_model(image_np)\n",
        "\n",
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "# TF Hub 사용법: 모델의 결과값을 dict 형태로 선언하여 사용\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "print(result.keys())    # 실제 값들이 어떻게 구성되어 있는지 보기 위한 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5VYaBoeeFM"
      },
      "source": [
        "## 결과 시각화하기\n",
        "\n",
        "여기에서는 추론 단계의 사각형(및 가능한 경우 키포인트)을 표시하기 위해 TensorFlow Object Detection API가 필요합니다.\n",
        "\n",
        "이 방법을 보여주는 전체 문서는 [여기](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)에서 확인할 수 있습니다.\n",
        "\n",
        "예를 들어, 여기에서 `min_score_thresh`를 다른 값(0과 1 사이)으로 설정하여 더 많이 감지할 수 있게 하거나 더 많은 감지를 필터링할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-07T18:27:58.535037Z",
          "iopub.status.busy": "2021-04-07T18:27:58.534446Z",
          "iopub.status.idle": "2021-04-07T18:27:59.807646Z",
          "shell.execute_reply": "2021-04-07T18:27:59.808085Z"
        },
        "id": "2O7rV8g9s8Bz"
      },
      "outputs": [],
      "source": [
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in result:\n",
        "  keypoints = result['detection_keypoints'][0]\n",
        "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections[0],\n",
        "              # 함수 내부에서 원본 이미지의 크기 확인 및 화면에 그리는 이미지 선언\n",
        "              # 모델 종류에 상관없이 이미지 크기는 원본 그대로 넣어도 된다.\n",
        "              # 자동으로 /255 계산과, resize 코드가 내장되어 있다.\n",
        "              # 원본 이미지값을 사용해야 나중에 화면에 표시할때 원본 크기로 계산할때 필요하기 때문\n",
        "      result['detection_boxes'][0],                                   # 사각형 좌표 들\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int), # Class 종류\n",
        "      result['detection_scores'][0],                                  # 각 사각형에 대한 object인 확률 (0~1 사이)\n",
        "      category_index,                                                 # category 종류를 넣어주면 출력한 class 숫자를 category_index[class]로 출력해준다.\n",
        "      use_normalized_coordinates=True,                                # max_boxes_to_draw와 min_score_thresh 사용여부\n",
        "      max_boxes_to_draw=200,                                          # 최대 200개까지 그림\n",
        "      min_score_thresh=.30,                                           # score값이 0.3이하면 제거\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenCV로 비디오 영상 읽기 쓰기 연습\n",
        "* 본 예제는 https://github.com/chulminkw/DLCV 에서 코드를 다운받아 수정된 코드임\n",
        "\n",
        "\n",
        "#### OpenCV 비디오 영상처리\n",
        "* OpenCV는 간편하게 비디오 영상처리를 할 수 있는 API를 제공\n",
        "* VideoCapture 객체는 Video Streaming을 Frame 별로 Capture하여 처리할 수 있는 기능 제공\n",
        "* VideoWriter 객체는 VideoCapture로 읽어들인 Frame을 동영상으로 Write하는 기능 제공"
      ],
      "metadata": {
        "id": "n1cZMrkc5lo8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6sPujNoAwSr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3QCc7qInm2V"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래 imoprt cv2 셀 실행 시, 에러 발생하면, opencv 버전 문제일 수 있음. 최신버전 uninstall 후 새로 설치\n",
        "pip uninstall opencv-python-headless==4.5.5.62"
      ],
      "metadata": {
        "id": "Tw8BEgRSEq2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python-headless==4.5.2.52"
      ],
      "metadata": {
        "id": "ThI-yoXxE5Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIA3E4uFA4yj"
      },
      "source": [
        "import cv2\n",
        "\n",
        "video_input_path = \"/content/drive/My Drive/Colab Notebooks/sample.mp4\"\n",
        "# linux에서 video output의 확장자는 반드시 avi 로 설정 필요. \n",
        "video_output_path = \"/content/drive/My Drive/Colab Notebooks/od_result.avi\"\n",
        "\n",
        "# 동영상 불러오기\n",
        "cap = cv2.VideoCapture(video_input_path)\n",
        "\n",
        "# avi 코덱 선언\n",
        "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
        "# 동영상 이미지 가로 크기\n",
        "width = round(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# 동영상 이미지 세로 크기\n",
        "height = round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "# 동영상 초당 영상 재생 개수\n",
        "vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "# 동영상 저장용 선언\n",
        "vid_writer = cv2.VideoWriter(video_output_path, codec, vid_fps, vid_size) \n",
        "\n",
        "# 동영상 총 이미지 갯수=총 frame 갯수\n",
        "frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print('총 Frame 갯수:', frame_cnt)\n",
        "print('fps:',vid_fps)\n",
        "print('총 시간:', int(frame_cnt/vid_fps),'초 = ', int(frame_cnt/vid_fps/60),'분', int(frame_cnt/vid_fps)%60,'초')\n",
        "print('가로:', width, '세로:', height)\n",
        "\n",
        "# 동영상에서 불필요한 장면을 스킵 하기 위해서 비디오 시작지점을 설정한다.\n",
        "# cap.set(cv2.CAP_PROP_POS_FRAMES, 40*vid_fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUToRZ8woxg1"
      },
      "source": [
        "import time"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MT0nH3fA9ji"
      },
      "source": [
        "def get_hub_model_result(image, _model):\n",
        "  # _model은 위에서 선택한 모델로 불러오도록 코드를 구성\n",
        "  # 선택한 모델로 찾을 image와 box와 labeling할 이미지  \n",
        "  start = time.time()\n",
        "  results = _model(image)\n",
        "  end = time.time()\n",
        "  print(end-start)\n",
        "\n",
        "  # different object detection models have additional results\n",
        "  # all of them are explained in the documentation\n",
        "  # tensorflow hub의 사용법은 모델의 결과값을 dict형태로 선언해서 사용하기 때문에 아래 형태로 선언\n",
        "  result = {key:value.numpy() for key,value in results.items()}\n",
        "  # 시작이 1 인지 0 인지 차이를 선언\n",
        "  label_id_offset = 0\n",
        "\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image[0],  # 함수 내부에서 원본 이미지의 크기 확인 및 화면에 그리는 이미지 선언\n",
        "                 # 모델 종류에 상관없이 이미지 크기는 원본 그대로 넣어도 된다.\n",
        "                 # 자동으로 /255 계산과, resize 코드가 내장되어 있다.\n",
        "                 # 원본 이미지값을 사용해야 나중에 화면에 표시할때 원본 크기로 계산할때 필요하기 때문\n",
        "      result['detection_boxes'][0],  # 사각형 좌표 들\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),  # class 종류\n",
        "      result['detection_scores'][0],  # 각 사각형에 대한 object인 확률 (0~1 사이)\n",
        "      category_index,  # category 종류를 넣어주면 출력한 class 숫자를 category_index[class]로 출력해준다.\n",
        "      use_normalized_coordinates=True, # max_boxes_to_draw와 min_score_thresh 사용여부\n",
        "      max_boxes_to_draw=200,  # 최대 200개 까지 그린다.\n",
        "      min_score_thresh=.30,  # score값이 0.3 이하는 제거\n",
        "      agnostic_mode=False,  # True일 경우 score만 표시, clas 표시 안함, 기본은 False\n",
        "      # 아래는 해당 값이 있으면 사용함\n",
        "   ) \n",
        "  return image[0]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX17KNLaFcwa"
      },
      "source": [
        "frame_count = 0\n",
        "while True:\n",
        "  hasFrame, img_frame = cap.read()\n",
        "  print('frame count', frame_count)  \n",
        "  if not hasFrame:\n",
        "    print('더 이상 처리할 frame이 없습니다.')\n",
        "    break  \n",
        "\n",
        "  # hub_model에서는 이미지는 4체널로 선언해줘야 하기 때문에 앞에 1차원을 만들어줌\n",
        "  test_img = np.array(img_frame).reshape((1, height, width, 3)).astype(np.uint8)\n",
        "  # test_img : 테스트할 이미지, hub_model : 테스트할 모델\n",
        "  draw_img_frame  = get_hub_model_result(test_img, hub_model)\n",
        "  # 동영상중에서 1개의 frame 이미지를 진행했다는 count 표시용\n",
        "  frame_count += 1\n",
        "\n",
        "  # 아래는 확인용 코드\n",
        "  # plt.figure(figsize=(24,32))  # 크게 보고싶으면 코드 주석 제거\n",
        "  # plt.imshow(draw_img_frame)\n",
        "  # plt.show()\n",
        "  vid_writer.write(draw_img_frame)  \n",
        "\n",
        "  # 너무 많이 저장하면 오래걸리기 때문에 짧은 시간안에 테스트 할정도로 설정\n",
        "  if frame_count > 100: break  \n",
        "\n",
        "vid_writer.release()\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf2_object_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}