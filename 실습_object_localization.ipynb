{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "실습-object_localization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN2c5idEb5-N"
      },
      "source": [
        "# Oxford Pet 데이터셋에 대한 Object Localization실습 예제\n",
        "본 코드는 \"텐서플로 딥러닝 프로그래밍\" (가메출판사) 책에 실려 있는 예제코드임."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuLishrPATac"
      },
      "source": [
        "# Oxford Pet 데이터셋 다운로드\n",
        "\n",
        "다음 사이트에 가서 이미지와 annotations을 다운로드받으면 됨\n",
        "이미지는 Dataset링크에서 받으면 되고, annotations은 Groundtruth data링크에서 받으면 됨.\n",
        "\n",
        "https://www.robots.ox.ac.uk/~vgg/data/pets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "uxo4Ms57VIJZ",
        "outputId": "6bfe7709-3243-4309-8a21-3a8296ea0398"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c3bce094-9621-4242-ba7b-8c49afc72dd0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c3bce094-9621-4242-ba7b-8c49afc72dd0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zcOSBydAp_4"
      },
      "source": [
        "#사이트에서 직접 데이터셋 다운로드\n",
        "#!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz #이미지를 다운로드\n",
        "#!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz #annotations를 다운로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hdYROeQOQgI"
      },
      "source": [
        "# images.tar.gz파일 앞축풀기\n",
        "!tar -xf images.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdWKJQzU155E"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cK-JcwD1-7P"
      },
      "source": [
        "# annotations.tar.gz파일 압축풀기\n",
        "!tar -xf annotations.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaGupsqTF2q3"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46hWTuDEeouF"
      },
      "source": [
        "# 기본 세팅\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image # pip install pillow\n",
        "import os\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2  # 만약 OpenCV가 설치되어 있지 않았다면 pip install opencv_python 로 설치를 먼저해야 함\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers   import Input, Dense, Flatten\n",
        "from tensorflow.keras.layers   import Conv2D, BatchNormalization,MaxPool2D, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJJwIQEeuiP"
      },
      "source": [
        "#GPU메모리 부족현상을 해결하기 위해\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzvQbyeoHOEx"
      },
      "source": [
        "#xml파일에서 Bounding Box를 추출하는 함수\n",
        "def getBB(file_path):\n",
        "  try:\n",
        "    tree = ET.parse(file_path)\n",
        "  except FileNotFoundError:\n",
        "    return None  \n",
        "  root = tree.getroot()  \n",
        "  ob = root.find('object')\n",
        "  bndbox = ob.find('bndbox')\n",
        "  xmin = bndbox.find('xmin').text\n",
        "  xmax = bndbox.find('xmax').text\n",
        "  ymin = bndbox.find('ymin').text\n",
        "  ymax = bndbox.find('ymax').text\n",
        "  return [int(xmin), int(ymin), int(xmax), int(ymax)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3KTl9D7fDS4"
      },
      "source": [
        "[Bounding Box from XML]<img src='https://drive.google.com/uc?id=12h7vEb0HLgHCbETgggQ70PVPDispKwEG'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06jkfdagMEw"
      },
      "source": [
        "[Bounding Box from XML]<img src='https://drive.google.com/uc?id=1T1489XS1Iq2RHYYFzwVD5kFHpYcsglHg'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyxVbFSkHggK"
      },
      "source": [
        "# oxford_pets 데이터파일을 열어서 test dataset과 training dataset만드는 함수\n",
        "def load_oxford_pets_3(target_size= (224, 224)):\n",
        "#    input_file = \"./Oxford_Pets/annotations/list.txt\"\n",
        "    input_file = \"./annotations/list.txt\"\n",
        "\n",
        "    file = open(input_file)\n",
        "    list_txt = file.readlines()\n",
        "    file.close()\n",
        "    \n",
        "    list_txt = list_txt[6:]     # delete header\n",
        "    np.random.shuffle(list_txt)\n",
        "\n",
        "    # load dataset\n",
        "    train_dataset= {\"name\": [], \"label\": [], \"image\": [ ], \"box\": [] }\n",
        "    test_dataset = {\"name\": [], \"label\": [], \"image\": [ ]}\n",
        "\n",
        "    for line in list_txt:    \n",
        "        image_name, class_id, species, breed_id = line.split()\n",
        "#        image_file= \"./Oxford_Pets/images/\"+ image_name + \".jpg\"\n",
        "#        box_file  = \"./Oxford_Pets/annotations/xmls/\"+ image_name + \".xml\"     \n",
        "        image_file= \"./images/\"+ image_name + \".jpg\"\n",
        "        box_file  = \"./annotations/xmls/\"+ image_name + \".xml\"     \n",
        "\n",
        "        if not os.path.exists(image_file):\n",
        "            continue\n",
        "\n",
        "        # read image and scale to target_size\n",
        "        img = image.load_img(image_file) # read as original size\n",
        "        sx = target_size[0]/img.width    # for rescaling BB\n",
        "        sy = target_size[1]/img.height\n",
        "            \n",
        "        img = img.resize(size=target_size)\n",
        "        img = image.img_to_array(img)  # (224, 224, 3)\n",
        "          \n",
        "        if  os.path.exists(box_file): # train_dataset\n",
        "            # read xml, rescale box by target_size   \n",
        "            box = getBB(box_file)\n",
        "            box[0] = round(box[0]*sx) # scale xmin with sx\n",
        "            box[1] = round(box[1]*sy) # scale ymin with sy\n",
        "            box[2] = round(box[2]*sx) # scale xmax with sx\n",
        "            box[3] = round(box[3]*sy) # scale ymax with sy\n",
        "            train_dataset[\"box\"].append(box)\n",
        "            train_dataset[\"name\"].append(image_name)\n",
        "            train_dataset[\"label\"].append(int(species)-1)\n",
        "            train_dataset[\"image\"].append(img)          \n",
        "            \n",
        "        else: #test_dataset\n",
        "            test_dataset[\"name\"].append(image_name)\n",
        "            test_dataset[\"label\"].append(int(species)-1)\n",
        "            test_dataset[\"image\"].append(img)        \n",
        "    # change list to np.array\n",
        "    train_dataset[\"image\"] = np.array(train_dataset[\"image\"])\n",
        "    train_dataset[\"box\"]  = np.array(train_dataset[\"box\"])\n",
        "    train_dataset[\"label\"] = np.array(train_dataset[\"label\"])\n",
        "    train_dataset[\"name\"]  = np.array(train_dataset[\"name\"])\n",
        "\n",
        "    test_dataset[\"image\"] = np.array(test_dataset[\"image\"])\n",
        "    test_dataset[\"label\"] = np.array(test_dataset[\"label\"])\n",
        "    test_dataset[\"name\"]  = np.array(test_dataset[\"name\"])    \n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcns52FgFfA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0qFj9D3HnKL"
      },
      "source": [
        "#load_oxford_pets_3()함수를 호출하여 train_dataset과 test_dataset만들기\n",
        "train_dataset, test_dataset = load_oxford_pets_3()\n",
        "print(\"train_dataset['image'].shape=\", train_dataset['image'].shape)# (5880, 224, 224, 3)\n",
        "\n",
        "#normalize \n",
        "x_train = train_dataset[\"image\"]/255.0\n",
        "y_train = train_dataset[\"box\"]/x_train.shape[1] # [0, 224] -> [0, 1]\n",
        "x_test  = test_dataset[\"image\"]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvzGEFMkH_Gq"
      },
      "source": [
        "# IOU를 계산하는 함수 \n",
        "def IOU(y_true, y_pred):   \n",
        "\n",
        "    b1_xmin, b1_ymin, b1_xmax, b1_ymax = tf.unstack(y_true, 4, axis=-1)\n",
        "    b2_xmin, b2_ymin, b2_xmax, b2_ymax = tf.unstack(y_pred, 4, axis=-1)\n",
        "\n",
        "    zero = tf.convert_to_tensor(0.0, y_true.dtype) # zero = 0.0      \n",
        "    b1_width  = tf.maximum(zero, b1_xmax - b1_xmin)\n",
        "    b1_height = tf.maximum(zero, b1_ymax - b1_ymin)\n",
        "    b2_width  = tf.maximum(zero, b2_xmax - b2_xmin)\n",
        "    b2_height = tf.maximum(zero, b2_ymax - b2_ymin)\n",
        "\n",
        "    b1_area = b1_width * b1_height\n",
        "    b2_area = b2_width * b2_height\n",
        "\n",
        "    intersect_ymin = tf.maximum(b1_ymin, b2_ymin)\n",
        "    intersect_xmin = tf.maximum(b1_xmin, b2_xmin)\n",
        "    intersect_ymax = tf.minimum(b1_ymax, b2_ymax)\n",
        "    intersect_xmax = tf.minimum(b1_xmax, b2_xmax)\n",
        "    \n",
        "    intersect_width = tf.maximum(zero, intersect_xmax - intersect_xmin)\n",
        "    intersect_height = tf.maximum(zero, intersect_ymax - intersect_ymin)\n",
        "    intersect_area = intersect_width * intersect_height\n",
        "\n",
        "    union_area = b1_area + b2_area - intersect_area\n",
        "    iou = intersect_area/union_area # tf.math.divide_no_nan(intersect_area, union_area)\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLJ8gcnkIKX5"
      },
      "source": [
        "#  cnn model을 만들기\n",
        "def create_cnn2d(input_shape, num_units = 4):\n",
        "    inputs = Input(shape = input_shape)\n",
        "    x = Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D()(x)\n",
        "\n",
        "    x = Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "    x = Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)    \n",
        "    x = MaxPool2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    outputs = tf.keras.layers.Dense(units = num_units, activation = 'sigmoid')(x)\n",
        "    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "    return model\n",
        "    \n",
        "model = create_cnn2d(input_shape = x_train.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35kS6dcNIW14"
      },
      "source": [
        "# 모델을 학습하기\n",
        "\n",
        "opt = RMSprop(learning_rate = 0.001)\n",
        "model.compile(optimizer = opt, loss = 'mse', metrics = [IOU])\n",
        "ret = model.fit(x_train, y_train, epochs =100, batch_size =128, verbose = 0)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "\n",
        "#6: plot accuracy and loss\n",
        "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
        "ax[0].plot(ret.history['loss'], \"g-\")\n",
        "ax[0].set_title('train loss')\n",
        "ax[0].set_xlabel('epochs')\n",
        "ax[0].set_ylabel('loss')\n",
        "\n",
        "ax[1].plot(ret.history['IOU'], \"b-\")\n",
        "ax[1].set_title('IOU')\n",
        "ax[1].set_xlabel('epochs')\n",
        "ax[1].set_ylabel('IOU')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7q2tVuVN3VW"
      },
      "source": [
        "# k개의 샘플에 대해 예측하고, 결과를 보여주기\n",
        "k = 8\n",
        "train_box = model.predict(x_train[:k])\n",
        "test_box = model.predict(x_test[:k])\n",
        "\n",
        "def display_images(img, pred_box, true_box=None, size= 224):\n",
        "    box = pred_box*size\n",
        "    box = box.astype(int)\n",
        "    k =  pred_box.shape[0]\n",
        "    fig = plt.figure(figsize=(8, k//2))\n",
        "\n",
        "    for i in range(k):\n",
        "        plt.subplot(k//4, 4, i+1)\n",
        "\n",
        "        a_img = (img[i]*255).astype('uint8')\n",
        "\n",
        "        # box predicted\n",
        "        xmin, ymin, xmax, ymax = box[i]           \n",
        "        cv2.rectangle(a_img, (xmin, ymin), (xmax, ymax), (0, 0, 255), 5)\n",
        "\n",
        "        if true_box is not None: # true box in case of train data\n",
        "            xmin, ymin, xmax, ymax = true_box[i]\n",
        "            cv2.rectangle(a_img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 5)\n",
        "        plt.imshow(a_img)\n",
        "        plt.axis(\"off\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_images(x_train[:k], train_box, train_dataset[\"box\"][:k])\n",
        "display_images(x_test[:k],  test_box)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}